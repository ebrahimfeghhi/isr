
Accuracy:  0.0
Batch number:  1000
Loss:  200.61668
Batch number:  2000
Loss:  107.28575
Batch number:  3000
Loss:  92.43141
Batch number:  4000
Loss:  87.78948
Batch number:  5000
Loss:  85.77209
Batch number:  6000
Loss:  84.0609
Batch number:  7000
Loss:  83.75044
Batch number:  8000
Loss:  82.01864
Batch number:  9000
Loss:  80.79407
Batch number:  10000
Loss:  79.94824
Accuracy:  0.0
Batch number:  11000
Loss:  78.38031
Batch number:  12000
Loss:  77.95543
Batch number:  13000
Loss:  76.59944
Batch number:  14000
Loss:  75.40069
Batch number:  15000
Loss:  74.3072
Batch number:  16000
Loss:  72.85962
Batch number:  17000
Loss:  71.88132
Batch number:  18000
Loss:  70.37608
Batch number:  19000
Loss:  69.57037
Batch number:  20000
Loss:  69.46668
Accuracy:  0.0
Batch number:  21000
Loss:  68.0155
Batch number:  22000
Loss:  67.46828
Batch number:  23000
Loss:  66.23518
Batch number:  24000
Loss:  65.14368
Batch number:  25000
Loss:  64.8923
Batch number:  26000
Loss:  64.72657
Batch number:  27000
Loss:  63.50557
Batch number:  28000
Loss:  62.67676
Batch number:  29000
Loss:  62.96563
Batch number:  30000
Loss:  62.4742
Accuracy:  0.0
Batch number:  31000
Loss:  60.45267
Batch number:  32000
Loss:  59.9478
Batch number:  33000
Loss:  59.78319
Batch number:  34000
Loss:  60.26915
Batch number:  35000
Loss:  58.71632
Batch number:  36000
Loss:  58.88853
Batch number:  37000
Loss:  57.74348
Batch number:  38000
Loss:  57.40667
Batch number:  39000
Loss:  56.40584
Batch number:  40000
Loss:  57.03259
Accuracy:  0.0
Batch number:  41000
Loss:  55.25069
Batch number:  42000
Loss:  56.06009
Batch number:  43000
Loss:  54.95665
Batch number:  44000
Loss:  54.46526
Batch number:  45000
Loss:  54.73143
Batch number:  46000
Loss:  53.73135
Batch number:  47000
Loss:  54.38335
Batch number:  48000
Loss:  53.2146
Batch number:  49000
Loss:  52.92494
Batch number:  50000
Loss:  52.98219
Accuracy:  0.0
Batch number:  51000
Loss:  52.82235
Batch number:  52000
Loss:  51.99524
Batch number:  53000
Loss:  52.00703
Batch number:  54000
Loss:  51.50687
Batch number:  55000
Loss:  51.90424
Batch number:  56000
Loss:  51.1405
Batch number:  57000
Loss:  51.63825
Batch number:  58000
Loss:  50.07642
Batch number:  59000
Loss:  50.83703
Batch number:  60000
Loss:  50.96343
Accuracy:  0.0
Batch number:  61000
Loss:  50.13604
Batch number:  62000
Loss:  50.11467
Batch number:  63000
Loss:  49.60062
Batch number:  64000
Loss:  48.908
Batch number:  65000
Loss:  49.4856
Batch number:  66000
Loss:  49.08596
Batch number:  67000
Loss:  48.24944
Batch number:  68000
Loss:  48.96164
Batch number:  69000
Loss:  49.29902
Batch number:  70000
Loss:  48.70632
Accuracy:  0.0002
Batch number:  71000
Loss:  48.92941
Batch number:  72000
Loss:  48.58134
Traceback (most recent call last):
  File "run.py", line 132, in <module>
    train_loop()
  File "run.py", line 83, in train_loop
    loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
Traceback (most recent call last):
  File "run.py", line 132, in <module>
    train_loop()
  File "run.py", line 83, in train_loop
    loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
